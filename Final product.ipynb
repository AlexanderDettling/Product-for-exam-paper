{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b535072e-e554-4496-b736-0c42f99fb09d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Data scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52166fc6-a352-4df8-8d3c-544095eb285a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To begin with, we install and import the twint library which we'll use to scrape Twitter data (tweets):\n",
    "\n",
    "%pip install --upgrade git+https://github.com/kevctae/twint.git \n",
    "import twint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb8d0d2-98ff-4ca4-ad0f-fe05f0174e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When using a Python notebook, this code is needed in order to avoid\n",
    "# compatibility issues with notebooks and RunTime errors:\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a52d8a1-99a5-404d-b753-2e9464a9065f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set the parameters of the search, specifying that we're interested \n",
    "# in Danish (da) tweets containing the hashtag \"#metoo\"\n",
    "\n",
    "\n",
    "config = twint.Config()\n",
    "config.Pandas = True\n",
    "config.Search = \"#metoo\"\n",
    "config.Lang = \"da\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b914c352-7950-4bc4-b1e1-6ceef305bff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we carry out the actual search for Danish tweets containing the hashtag \"#metoo\":\n",
    "\n",
    "twint.run.Search(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1cc256-3845-4808-a37d-c0132ccf1487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We install and import the pandas library in order to be able to save the tweets in a dataframe:\n",
    "\n",
    "%pip install pandas\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64549a3e-ecc2-4892-ae33-27759796e72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This stores all the tweets + metainformation (username, time and date of tweets etc.) in a pandas dataframe:\n",
    "\n",
    "df = twint.storage.panda.Tweets_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6da92a-3c61-4c6d-a30e-2439bfad8fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we save the dataframe as a csv file:\n",
    "\n",
    "df.to_csv('twitter_corpus.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3d4866-8dae-47fd-8421-ba5667820601",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6de1fc7-1e30-483d-8b9a-940e87eb8656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We install and import the necessary libraries needed to clean the tweets, including the \n",
    "# Danish lemmatization module \"da_core_news_lg\":\n",
    "\n",
    "%pip install pandas\n",
    "%pip install spacy\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "!python3 -m spacy download da_core_news_lg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e8973b1-4270-4aeb-874c-8871062740bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df is the dataframe containing (among other things) the uncleaned tweets\n",
    "\n",
    "df = pd.read_csv('twitter_corpus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abd43ac0-1d37-4939-96fd-cb4503e08d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a function that uses regular expressions to remove everything from the tweets \n",
    "# that we won't need in the actual topic modelling:\n",
    "\n",
    "\n",
    "def remove_unnecessary_stuff(tweet):\n",
    "\n",
    "    tweet = re.sub('@[^\\s]+','',tweet)    # This removes all tags (where other Twitter users have been tagged)\n",
    "    tweet = re.sub('http[^\\s]+','',tweet) # This removes all links\n",
    "    tweet = re.sub('#[^\\s]+','',tweet)    # This removes all hashtags\n",
    "    tweet = re.sub(r'[^\\w\\s]', '', tweet) # This removes everything that's not a letter or a number\n",
    "    tweet = re.sub(' +',' ', tweet)       # This converts all cases of more than 1 whitespace into just 1 whitespace\n",
    "    tweet = tweet.strip()                 # This deletes all whitespaces at the beginning and end of tweets\n",
    "    \n",
    "    return tweet  \n",
    "\n",
    "\n",
    "# We define a function that lemmatizes all words in all tweets. \n",
    "# To do so, we need to first load the Danish lemmatization module \"da_core_news_lg\":\n",
    "\n",
    "lemmas = spacy.load(\"da_core_news_lg\")\n",
    "\n",
    "def lemmatizer(tweet):\n",
    "    tweet = lemmas(tweet)\n",
    "  \n",
    "    return \" \".join([word.lemma_ for word in tweet]) \n",
    "\n",
    "\n",
    "\n",
    "# We now apply the function that removes all unnecessary stuff to the dataframe column containing the tweets:\n",
    "\n",
    "df['tweet'] = df['tweet'].apply(remove_unnecessary_stuff)\n",
    "\n",
    "\n",
    "# Then we'll apply the lemmatizing function to the tweets:\n",
    "\n",
    "df['tweet'] = df['tweet'].apply(lemmatizer)\n",
    "\n",
    "\n",
    "# Finally, we'll apply a function to our tweets which will remove all Danish stopwords from the tweets and make\n",
    "# all tweets lowercase. To do so, we first need to import a set of Danish stopwords from the spaCy library:\n",
    "\n",
    "from spacy.lang.da.stop_words import STOP_WORDS\n",
    "\n",
    "df['tweet'] = df['tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in STOP_WORDS]).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdf4535d-4d3a-42e8-9b5c-2931e02b3115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will save the new, cleaned dataset as a csv file\n",
    "\n",
    "df.to_csv('cleaned_twitter_corpus_lemmatized_without_stopwords.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b213a91-6d0e-4a05-888f-04152daf19f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Creating a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccbdb8d-5695-46f2-ab2e-316f2b51ca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We install and import the necessary libraries needed to make our pipeline\n",
    "\n",
    "%pip install tweetopic\n",
    "%pip install pandas\n",
    "%pip install numpy\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1623190f-e709-40f2-8f54-808c63a3fdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make a vectorizer to turn our corpus of tweets into a doc-term matrix\n",
    "# min_df=5 means that words that appear in less than 5 tweets will be ignored\n",
    "# max_df=0.9 means that words that appear in more than 90 % of the tweets will be ignored\n",
    "\n",
    "from tweetopic import DMM\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=5, max_df=0.5)\n",
    "\n",
    "\n",
    "# We specify that we want a topic model with 6 components (topics):\n",
    "\n",
    "dmm = DMM(n_components=6, alpha=0.1, beta=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cca229eb-5c79-4643-8f07-d09d0203dfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create the actual topic pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"vectorizer\", vectorizer),\n",
    "    (\"dmm\", dmm),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4d9702e8-f7bd-484b-b555-6e1cd583e1fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We load the cleaned twitter corpus and save it in the variable \"texts\"\n",
    "\n",
    "df=pd.read_csv('cleaned_twitter_corpus_lemmatized_without_stopwords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c5dc2c70-a520-4ea0-8325-cfc87720f159",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here we first convert the dataframe column with the tweets into a list, \n",
    "# then we convert the list into a NumPy array. This is necessary because the pipeline will\n",
    "# not accept a list or a dataframe column as input:\n",
    "\n",
    "tweets=np.array(list(df['tweet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd115af-b34f-4df1-a403-42471b147483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can run the actual pipeline on our cleaned tweets:\n",
    "\n",
    "pipeline.fit(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecf8714-c34b-4180-b2bf-017cd3417fc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Visualization of topic modelling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7100d5-7862-4561-9684-2c0f1c550723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to be able to visualize our topic modelling via topic-wizard, we must\n",
    "# install and import the newest version (1.10.1) of the SciPy library:\n",
    "\n",
    "!pip install scipy==1.10.1\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f429f90d-e0bd-4f9d-9265-8812f6e01de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We install and import the topic-wizard library\n",
    "\n",
    "%pip install topic-wizard\n",
    "import topicwizard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6105d42-07b4-4959-8dd7-4022b5705661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we're now able to visualize our topic modelling:\n",
    "\n",
    "topicwizard.visualize(pipeline=pipeline, corpus=tweets,port=1131)\n",
    "\n",
    "\n",
    "# Notice that this code should be run in Anaconda as the visualization \n",
    "# will likely not work when the code is run in Ucloud or Google Colab."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
